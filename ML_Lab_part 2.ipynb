{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxhCw3gVpkbM",
        "outputId": "168562bb-e746-4b11-ea54-350a456e0a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "a-nkcJWl5pVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Calculating Splits*"
      ],
      "metadata": {
        "id": "r2o73qYw5yIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best split point for a dataset\n",
        "def get_split(dataset, n_features):\n",
        "  class_values = list(set(row[-1] for row in dataset))\n",
        "  b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "  features = list()\n",
        "  while len(features) < n_features:\n",
        "    index = randrange(len(dataset[0])-1)\n",
        "    if index not in features:\n",
        "      features.append(index)\n",
        "      for index in features:\n",
        "        for row in dataset:\n",
        "          groups = test_split(index, row[index], dataset)\n",
        "          gini = gini_index(groups, class_values)\n",
        "          if gini < b_score:\n",
        "            b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "      return {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "metadata": {
        "id": "UstwnvYFB0sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Random Forest algorithm on sonar dataset*"
      ],
      "metadata": {
        "id": "l27YfrSF7UOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = len(dataset) // n_folds\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "    gini = 0.0\n",
        "    for class_value in class_values:\n",
        "        for group in groups:\n",
        "            size = len(group)\n",
        "            if size == 0:\n",
        "                continue\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "            gini += (proportion * (1.0 - proportion))\n",
        "    return gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset, n_features):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    features = list()\n",
        "    while len(features) < n_features:\n",
        "        index = randrange(len(dataset[0])-1)\n",
        "        if index not in features:\n",
        "            features.append(index)\n",
        "    for index in features:\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, n_features, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left, n_features)\n",
        "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "    # process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right, n_features)\n",
        "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size, n_features):\n",
        "    root = get_split(train, n_features)\n",
        "    split(root, max_depth, min_size, n_features, 1)\n",
        "    return root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "    sample = list()\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    while len(sample) < n_sample:\n",
        "        index = randrange(len(dataset))\n",
        "        sample.append(dataset[index])\n",
        "    return sample\n",
        "\n",
        "# Make a prediction with a list of bagged trees\n",
        "def bagging_predict(trees, row):\n",
        "    predictions = [predict(tree, row) for tree in trees]\n",
        "    return max(set(predictions), key=predictions.count)\n",
        "\n",
        "# Random Forest Algorithm\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "    trees = list()\n",
        "    for _ in range(n_trees):\n",
        "        sample = subsample(train, sample_size)\n",
        "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
        "        trees.append(tree)\n",
        "    predictions = [bagging_predict(trees, row) for row in test]\n",
        "    return predictions\n",
        "\n",
        "# Test the random forest algorithm on sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = '/content/drive/MyDrive/ML_Lab-04/sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "    str_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "\n",
        "for n_trees in [1, 5, 10]:\n",
        "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size,\n",
        "                                 sample_size, n_trees, n_features)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "id": "Vt_I9tJoB0ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ff50d8-523f-4e49-eac5-9a5ee8f1948b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trees: 1\n",
            "Scores: [51.21951219512195, 78.04878048780488, 58.536585365853654, 65.85365853658537, 53.65853658536586]\n",
            "Mean Accuracy: 61.463%\n",
            "Trees: 5\n",
            "Scores: [63.41463414634146, 60.97560975609756, 56.09756097560976, 60.97560975609756, 56.09756097560976]\n",
            "Mean Accuracy: 59.512%\n",
            "Trees: 10\n",
            "Scores: [65.85365853658537, 58.536585365853654, 68.29268292682927, 53.65853658536586, 75.60975609756098]\n",
            "Mean Accuracy: 64.390%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Chapter 11*"
      ],
      "metadata": {
        "id": "KHvR9Oindbk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "  gini = 0.0\n",
        "  for class_value in class_values:\n",
        "    for group in groups:\n",
        "      size = len(group)\n",
        "      if size == 0:\n",
        "        continue\n",
        "      proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "      gini += (proportion * (1.0 - proportion))\n",
        "  return gini"
      ],
      "metadata": {
        "id": "s9Zn3iKzAak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of calculatin Gini index\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "  gini = 0.0\n",
        "  for class_value in class_values:\n",
        "    for group in groups:\n",
        "      size = len(group)\n",
        "      if size == 0:\n",
        "        continue\n",
        "        proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "        gini += (proportion * (1.0 - proportion))\n",
        "  return gini\n",
        "\n",
        "# test Gini values\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2l0Mu4NeTpC",
        "outputId": "811f0730-3dcf-4503-ee4d-bbaaeb2674e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "  left, right = list(), list()\n",
        "  for row in dataset:\n",
        "    if row[index] < value:\n",
        "      left.append(row)\n",
        "    else:\n",
        "      right.append(row)\n",
        "  return left, right"
      ],
      "metadata": {
        "id": "EiNH9U6SeoDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "  class_values = list(set(row[-1] for row in dataset))\n",
        "  b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "  for index in range(len(dataset[0])-1):\n",
        "    for row in dataset:\n",
        "      groups = test_split(index, row[index], dataset)\n",
        "      gini = gini_index(groups, class_values)\n",
        "      if gini < b_score:\n",
        "        b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "  return {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "metadata": {
        "id": "Xoxx53I1evZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of getting the best split\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "  left, right = list(), list()\n",
        "  for row in dataset:\n",
        "    if row[index] < value:\n",
        "      left.append(row)\n",
        "    else:\n",
        "      right.append(row)\n",
        "  return left, right\n",
        "\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "  gini = 0.0\n",
        "  for class_value in class_values:\n",
        "    for group in groups:\n",
        "      size = len(group)\n",
        "      if size == 0:\n",
        "        continue\n",
        "      proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "      gini += (proportion * (1.0 - proportion))\n",
        "  return gini\n",
        "\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "  class_values = list(set(row[-1] for row in dataset))\n",
        "  b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "  for index in range(len(dataset[0])-1):\n",
        "    for row in dataset:\n",
        "      groups = test_split(index, row[index], dataset)\n",
        "      gini = gini_index(groups, class_values)\n",
        "      print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
        "      if gini < b_score:\n",
        "        b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "  return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "\n",
        "\n",
        "# Test getting the best split\n",
        "dataset = [[2.771244718,1.784783929,0],\n",
        "[1.728571309,1.169761413,0],\n",
        "[3.678319846,2.81281357,0],\n",
        "[3.961043357,2.61995032,0],\n",
        "[2.999208922,2.209014212,0],\n",
        "[7.497545867,3.162953546,1],\n",
        "[9.00220326,3.339047188,1],\n",
        "[7.444542326,0.476683375,1],\n",
        "[10.12493903,3.234550982,1],\n",
        "[6.642287351,3.319983761,1]]\n",
        "split = get_split(dataset)\n",
        "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl3LcVaue5LS",
        "outputId": "19de1cde-ef4a-496a-e9ab-a54fdf4da412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 < 2.771 Gini=0.494\n",
            "X1 < 1.729 Gini=0.500\n",
            "X1 < 3.678 Gini=0.408\n",
            "X1 < 3.961 Gini=0.278\n",
            "X1 < 2.999 Gini=0.469\n",
            "X1 < 7.498 Gini=0.408\n",
            "X1 < 9.002 Gini=0.469\n",
            "X1 < 7.445 Gini=0.278\n",
            "X1 < 10.125 Gini=0.494\n",
            "X1 < 6.642 Gini=0.000\n",
            "X2 < 1.785 Gini=1.000\n",
            "X2 < 1.170 Gini=0.494\n",
            "X2 < 2.813 Gini=0.640\n",
            "X2 < 2.620 Gini=0.819\n",
            "X2 < 2.209 Gini=0.934\n",
            "X2 < 3.163 Gini=0.278\n",
            "X2 < 3.339 Gini=0.494\n",
            "X2 < 0.477 Gini=0.500\n",
            "X2 < 3.235 Gini=0.408\n",
            "X2 < 3.320 Gini=0.469\n",
            "Split: [X1 < 6.642]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "  outcomes = [row[-1] for row in group]\n",
        "  return max(set(outcomes), key=outcomes.count)"
      ],
      "metadata": {
        "id": "y1n9SDVkfgVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "  left, right = node['groups']\n",
        "  del(node['groups'])\n",
        "  # check for a no split\n",
        "  if not left or not right:\n",
        "    node['left'] = node['right'] = to_terminal(left + right)\n",
        "    return\n",
        "    # check for max depth\n",
        "  if depth >= max_depth:\n",
        "    node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "    return\n",
        "    # process left child\n",
        "  if len(left) <= min_size:\n",
        "    node['left'] = to_terminal(left)\n",
        "  else:\n",
        "    node['left'] = get_split(left)\n",
        "    split(node['left'], max_depth, min_size, depth+1)\n",
        "    # process right child\n",
        "  if len(right) <= min_size:\n",
        "    node['right'] = to_terminal(right)\n",
        "  else:\n",
        "    node['right'] = get_split(right)\n",
        "    split(node['right'], max_depth, min_size, depth+1)"
      ],
      "metadata": {
        "id": "E3kfJltMfoCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "  root = get_split(dataset)\n",
        "  split(root, max_depth, min_size, 1)\n",
        "  return root"
      ],
      "metadata": {
        "id": "RPuVHeYUf67e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(node, depth=0):\n",
        "  if isinstance(node, dict):\n",
        "    print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "    print_tree(node['left'], depth+1)\n",
        "    print_tree(node['right'], depth+1)\n",
        "  else:\n",
        "    print('%s[%s]' % ((depth*' ', node)))"
      ],
      "metadata": {
        "id": "EBqoFf-JgA67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [[2.771244718,1.784783929,0],\n",
        "[1.728571309,1.169761413,0],\n",
        "[3.678319846,2.81281357,0],\n",
        "[3.961043357,2.61995032,0],\n",
        "[2.999208922,2.209014212,0],\n",
        "[7.497545867,3.162953546,1],\n",
        "[9.00220326,3.339047188,1],\n",
        "[7.444542326,0.476683375,1],\n",
        "[10.12493903,3.234550982,1],\n",
        "[6.642287351,3.319983761,1]]\n",
        "tree = build_tree(dataset, 1, 1)\n",
        "print_tree(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3LiRWkdgI4t",
        "outputId": "729f9864-eeea-4ced-f95d-0b9a49f503f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 < 2.771 Gini=0.494\n",
            "X1 < 1.729 Gini=0.500\n",
            "X1 < 3.678 Gini=0.408\n",
            "X1 < 3.961 Gini=0.278\n",
            "X1 < 2.999 Gini=0.469\n",
            "X1 < 7.498 Gini=0.408\n",
            "X1 < 9.002 Gini=0.469\n",
            "X1 < 7.445 Gini=0.278\n",
            "X1 < 10.125 Gini=0.494\n",
            "X1 < 6.642 Gini=0.000\n",
            "X2 < 1.785 Gini=1.000\n",
            "X2 < 1.170 Gini=0.494\n",
            "X2 < 2.813 Gini=0.640\n",
            "X2 < 2.620 Gini=0.819\n",
            "X2 < 2.209 Gini=0.934\n",
            "X2 < 3.163 Gini=0.278\n",
            "X2 < 3.339 Gini=0.494\n",
            "X2 < 0.477 Gini=0.500\n",
            "X2 < 3.235 Gini=0.408\n",
            "X2 < 3.320 Gini=0.469\n",
            "[X1 < 6.642]\n",
            " [0]\n",
            " [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "  if row[node['index']] < node['value']:\n",
        "    if isinstance(node['left'], dict):\n",
        "      return predict(node['left'], row)\n",
        "    else:\n",
        "      return node['left']\n",
        "  else:\n",
        "    if isinstance(node['right'], dict):\n",
        "      return predict(node['right'], row)\n",
        "    else:\n",
        "      return node['right']"
      ],
      "metadata": {
        "id": "zCtke2b3gMyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [[2.771244718,1.784783929,0],[1.728571309,1.169761413,0],\n",
        "[3.678319846,2.81281357,0],\n",
        "[3.961043357,2.61995032,0],\n",
        "[2.999208922,2.209014212,0],\n",
        "[7.497545867,3.162953546,1],\n",
        "[9.00220326,3.339047188,1],\n",
        "[7.444542326,0.476683375,1],\n",
        "[10.12493903,3.234550982,1],\n",
        "[6.642287351,3.319983761,1]]\n",
        "# predict with a stump\n",
        "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}\n",
        "for row in dataset:\n",
        "  prediction = predict(stump, row)\n",
        "  print('Expected=%d, Got=%d' % (row[-1], prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQD2cYEJgYYH",
        "outputId": "2cc50ef9-3efd-4b03-f0c0-c3247726467d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange\n",
        "from csv import reader\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "    gini = 0.0\n",
        "    for class_value in class_values:\n",
        "        for group in groups:\n",
        "            size = len(group)\n",
        "            if size == 0:\n",
        "                continue\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "            gini += (proportion * (1.0 - proportion))\n",
        "    return gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    # process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Classification and Regression Tree Algorithm\n",
        "def decision_tree(train, test, max_depth, min_size):\n",
        "    tree = build_tree(train, max_depth, min_size)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        prediction = predict(tree, row)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Test CART on Bank Note dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = '/content/drive/MyDrive/ML_Lab-04/data_banknote_authentication.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(len(dataset[0]) - 1):\n",
        "    str_column_to_float(dataset, i)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhXSZWcKhMOG",
        "outputId": "ef034ab0-4e5a-45de-9b81-f2873c44ee94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [83.57664233576642, 82.84671532846716, 86.86131386861314, 79.92700729927007, 82.11678832116789]\n",
            "Mean Accuracy: 83.066%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Chapeter 12*"
      ],
      "metadata": {
        "id": "iVBHPNWnjaj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset by class values, returns a dictionary\n",
        "def separate_by_class(dataset):\n",
        "  separated = dict()\n",
        "  for i in range(len(dataset)):\n",
        "    vector = dataset[i]\n",
        "    class_value = vector[-1]\n",
        "    if (class_value not in separated):\n",
        "      separated[class_value] = list()\n",
        "    separated[class_value].append(vector)\n",
        "  return separated"
      ],
      "metadata": {
        "id": "TwKZnJ_ZjTtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test separating data by class\n",
        "dataset = [[3.393533211,2.331273381,0],\n",
        "[3.110073483,1.781539638,0],\n",
        "[1.343808831,3.368360954,0],\n",
        "[3.582294042,4.67917911,0],\n",
        "[2.280362439,2.866990263,0],\n",
        "[7.423436942,4.696522875,1],\n",
        "[5.745051997,3.533989803,1],\n",
        "[9.172168622,2.511101045,1],\n",
        "[7.792783481,3.424088941,1],\n",
        "[7.939820817,0.791637231,1]]\n",
        "separated = separate_by_class(dataset)\n",
        "for label in separated:\n",
        "  print(label)\n",
        "  for row in separated[label]:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HBz4mOOjnzO",
        "outputId": "38bb7067-e355-48c8-87b5-b94c4fe5f86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[3.393533211, 2.331273381, 0]\n",
            "[3.110073483, 1.781539638, 0]\n",
            "[1.343808831, 3.368360954, 0]\n",
            "[3.582294042, 4.67917911, 0]\n",
            "[2.280362439, 2.866990263, 0]\n",
            "1\n",
            "[7.423436942, 4.696522875, 1]\n",
            "[5.745051997, 3.533989803, 1]\n",
            "[9.172168622, 2.511101045, 1]\n",
            "[7.792783481, 3.424088941, 1]\n",
            "[7.939820817, 0.791637231, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean of a list of numbers\n",
        "def mean(numbers):\n",
        "  return sum(numbers)/float(len(numbers))"
      ],
      "metadata": {
        "id": "-wyLvHN-juxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "# Calculate the standard deviation of a list of numbers\n",
        "def stdev(numbers):\n",
        "  avg = mean(numbers)\n",
        "  variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
        "  return sqrt(variance)\n",
        "\n",
        "def summarize_dataset(dataset):\n",
        "  summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "  del(summaries[-1])\n",
        "  return summaries\n",
        "\n",
        "\n",
        "# Test summarizing a dataset\n",
        "dataset = [[3.393533211,2.331273381,0],\n",
        "[3.110073483,1.781539638,0],\n",
        "[1.343808831,3.368360954,0],\n",
        "[3.582294042,4.67917911,0],\n",
        "[2.280362439,2.866990263,0],\n",
        "[7.423436942,4.696522875,1],\n",
        "[5.745051997,3.533989803,1],\n",
        "[9.172168622,2.511101045,1],\n",
        "[7.792783481,3.424088941,1],\n",
        "[7.939820817,0.791637231,1]]\n",
        "summary = summarize_dataset(dataset)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDFHHueYjxrK",
        "outputId": "c0843ca1-9b1d-40d6-c285-22d689509be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_by_class(dataset):\n",
        "  separated = dict()\n",
        "  for i in range(len(dataset)):\n",
        "    vector = dataset[i]\n",
        "    class_value = vector[-1]\n",
        "    if (class_value not in separated):\n",
        "      separated[class_value] = list()\n",
        "    separated[class_value].append(vector)\n",
        "  return separated\n",
        "\n",
        "# Split dataset by class then calculate statistics for each row\n",
        "def summarize_by_class(dataset):\n",
        "  separated = separate_by_class(dataset)\n",
        "  summaries = dict()\n",
        "  for class_value, rows in separated.iteritems():\n",
        "    summaries[class_value] = summarize_dataset(rows)\n",
        "  return summaries"
      ],
      "metadata": {
        "id": "qFNy6iETkCaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "# Split the dataset by class values, returns a dictionary\n",
        "def separate_by_class(dataset):\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        class_value = vector[-1]\n",
        "        if class_value not in separated:\n",
        "            separated[class_value] = list()\n",
        "        separated[class_value].append(vector)\n",
        "    return separated\n",
        "\n",
        "# Calculate the mean of a list of numbers\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "# Calculate the standard deviation of a list of numbers\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([(x - avg)**2 for x in numbers]) / float(len(numbers) - 1)\n",
        "    return sqrt(variance)\n",
        "\n",
        "# Calculate the mean, stdev, and count for each column in a dataset\n",
        "def summarize_dataset(dataset):\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "# Split dataset by class then calculate statistics for each row\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = dict()\n",
        "    for class_value, rows in separated.items():\n",
        "        summaries[class_value] = summarize_dataset(rows)\n",
        "    return summaries\n",
        "\n",
        "# Test summarizing by class\n",
        "dataset = [\n",
        "    [3.393533211, 2.331273381, 0],\n",
        "    [3.110073483, 1.781539638, 0],\n",
        "    [1.343808831, 3.368360954, 0],\n",
        "    [3.582294042, 4.67917911, 0],\n",
        "    [2.280362439, 2.866990263, 0],\n",
        "    [7.423436942, 4.696522875, 1],\n",
        "    [5.745051997, 3.533989803, 1],\n",
        "    [9.172168622, 2.511101045, 1],\n",
        "    [7.792783481, 3.424088941, 1],\n",
        "    [7.939820817, 0.791637231, 1]\n",
        "]\n",
        "\n",
        "summary = summarize_by_class(dataset)\n",
        "for label in summary:\n",
        "    print(label)\n",
        "    for row in summary[label]:\n",
        "        print(row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5oDZrBwr6gJ",
        "outputId": "419fe22b-3bbc-457c-b20e-24564ad07db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(2.7420144012, 0.9265683289298018, 5)\n",
            "(3.0054686692, 1.1073295894898725, 5)\n",
            "1\n",
            "(7.6146523718, 1.2344321550313704, 5)\n",
            "(2.9914679790000003, 1.4541931384601618, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Gaussian probability distribution function for x\n",
        "def calculate_probability(x, mean, stdev):\n",
        "  exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
        "  return (1 / (sqrt(2 * pi) * stdev)) * exponent"
      ],
      "metadata": {
        "id": "OPcJzmbMsits"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Gaussian PDF\n",
        "from math import sqrt\n",
        "from math import pi\n",
        "from math import exp\n",
        "# Calculate the Gaussian probability distribution function for x\n",
        "def calculate_probability(x, mean, stdev):\n",
        "  exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
        "  return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "\n",
        "# Test Gaussian PDF\n",
        "print(calculate_probability(1.0, 1.0, 1.0))\n",
        "print(calculate_probability(2.0, 1.0, 1.0))\n",
        "print(calculate_probability(0.0, 1.0, 1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxVYYDoTsnKo",
        "outputId": "3a8615ad-ef23-4700-f942-f878661086ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3989422804014327\n",
            "0.24197072451914337\n",
            "0.24197072451914337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probabilities of predicting each class for a given row\n",
        "def calculate_class_probabilities(summaries, row):\n",
        "  total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "  probabilities = dict()\n",
        "  for class_value, class_summaries in summaries.iteritems():\n",
        "    probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
        "    for i in range(len(class_summaries)):\n",
        "      mean, stdev, count = class_summaries[i]\n",
        "      probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "9vNc-uwBkuKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt, pi, exp\n",
        "\n",
        "# Split the dataset by class values, returns a dictionary\n",
        "def separate_by_class(dataset):\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        class_value = vector[-1]\n",
        "        if class_value not in separated:\n",
        "            separated[class_value] = list()\n",
        "        separated[class_value].append(vector)\n",
        "    return separated\n",
        "\n",
        "# Calculate the mean of a list of numbers\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "# Calculate the standard deviation of a list of numbers\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([(x - avg)**2 for x in numbers]) / float(len(numbers) - 1)\n",
        "    return sqrt(variance)\n",
        "\n",
        "# Calculate the mean, stdev, and count for each column in a dataset\n",
        "def summarize_dataset(dataset):\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "# Split dataset by class then calculate statistics for each row\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = dict()\n",
        "    for class_value, rows in separated.items():\n",
        "        summaries[class_value] = summarize_dataset(rows)\n",
        "    return summaries\n",
        "\n",
        "# Calculate the Gaussian probability distribution function for x\n",
        "def calculate_probability(x, mean, stdev):\n",
        "    exponent = exp(-((x - mean)**2 / (2 * stdev**2)))\n",
        "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "\n",
        "# Calculate the probabilities of predicting each class for a given row\n",
        "def calculate_class_probabilities(summaries, row):\n",
        "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "    probabilities = dict()\n",
        "    for class_value, class_summaries in summaries.items():\n",
        "        probabilities[class_value] = summaries[class_value][0][2] / float(total_rows)\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, stdev, count = class_summaries[i]\n",
        "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "    return probabilities\n",
        "\n",
        "# Test calculating class probabilities\n",
        "dataset = [\n",
        "    [3.393533211, 2.331273381, 0],\n",
        "    [3.110073483, 1.781539638, 0],\n",
        "    [1.343808831, 3.368360954, 0],\n",
        "    [3.582294042, 4.67917911, 0],\n",
        "    [2.280362439, 2.866990263, 0],\n",
        "    [7.423436942, 4.696522875, 1],\n",
        "    [5.745051997, 3.533989803, 1],\n",
        "    [9.172168622, 2.511101045, 1],\n",
        "    [7.792783481, 3.424088941, 1],\n",
        "    [7.939820817, 0.791637231, 1]\n",
        "]\n",
        "\n",
        "summaries = summarize_by_class(dataset)\n",
        "probabilities = calculate_class_probabilities(summaries, dataset[0])\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGzdJrrystp0",
        "outputId": "3ebf209d-a39c-44a7-8f85-433b33b20ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.05032427673372076, 1: 0.00011557718379945765}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "from random import randrange\n",
        "from math import sqrt, exp, pi\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for i, row in enumerate(csv_reader):\n",
        "            if i == 0:\n",
        "                continue  # Skip the header row\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = len(dataset) // n_folds\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross-validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Split the dataset by class values, returns a dictionary\n",
        "def separate_by_class(dataset):\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        class_value = vector[-1]\n",
        "        if class_value not in separated:\n",
        "            separated[class_value] = list()\n",
        "        separated[class_value].append(vector)\n",
        "    return separated\n",
        "\n",
        "# Calculate the mean of a list of numbers\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "# Calculate the standard deviation of a list of numbers\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([(x - avg)**2 for x in numbers]) / float(len(numbers) - 1)\n",
        "    return sqrt(variance)\n",
        "\n",
        "# Calculate the mean, stdev, and count for each column in a dataset\n",
        "def summarize_dataset(dataset):\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "# Split dataset by class then calculate statistics for each row\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = dict()\n",
        "    for class_value, rows in separated.items():\n",
        "        summaries[class_value] = summarize_dataset(rows)\n",
        "    return summaries\n",
        "\n",
        "# Calculate the Gaussian probability distribution function for x\n",
        "def calculate_probability(x, mean, stdev):\n",
        "    exponent = exp(-((x - mean)**2 / (2 * stdev**2)))\n",
        "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "\n",
        "# Calculate the probabilities of predicting each class for a given row\n",
        "def calculate_class_probabilities(summaries, row):\n",
        "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "    probabilities = dict()\n",
        "    for class_value, class_summaries in summaries.items():\n",
        "        probabilities[class_value] = summaries[class_value][0][2] / float(total_rows)\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, stdev, count = class_summaries[i]\n",
        "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "    return probabilities\n",
        "\n",
        "# Predict the class for a given row\n",
        "def predict(summaries, row):\n",
        "    probabilities = calculate_class_probabilities(summaries, row)\n",
        "    best_label, best_prob = None, -1\n",
        "    for class_value, probability in probabilities.items():\n",
        "        if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "    return best_label\n",
        "\n",
        "# Naive Bayes Algorithm\n",
        "def naive_bayes(train, test):\n",
        "    summarize = summarize_by_class(train)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        output = predict(summarize, row)\n",
        "        predictions.append(output)\n",
        "    return predictions\n",
        "\n",
        "# Test Naive Bayes on Iris Dataset\n",
        "filename = '/content/drive/MyDrive/ML_Lab-04/Iris.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "    str_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePgSstVrtA-w",
        "outputId": "9d03ea11-1b99-4052-bca6-5eb04f0cad23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [100.0, 100.0, 96.66666666666667, 100.0, 100.0]\n",
            "Mean Accuracy: 99.333%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Chapter 16*"
      ],
      "metadata": {
        "id": "h8oTZL2EuJxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio=1.0):\n",
        "  sample = list()\n",
        "  n_sample = round(len(dataset) * ratio)\n",
        "  while len(sample) < n_sample:\n",
        "    index = randrange(len(dataset))\n",
        "    sample.append(dataset[index])\n",
        "  return sample"
      ],
      "metadata": {
        "id": "KBkQl_CztkYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, random, randrange\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio=1.0):\n",
        "    sample = list()\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    while len(sample) < n_sample:\n",
        "        index = randrange(len(dataset))\n",
        "        sample.append(dataset[index])\n",
        "    return sample\n",
        "\n",
        "# Calculate the mean of a list of numbers\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "# Test subsampling a dataset\n",
        "seed(1)\n",
        "\n",
        "# True mean\n",
        "dataset = [[randrange(10)] for i in range(20)]\n",
        "print('True Mean: %.3f' % mean([row[0] for row in dataset]))\n",
        "\n",
        "# Estimated means\n",
        "ratio = 0.10\n",
        "for size in [1, 10, 100]:\n",
        "    sample_means = list()\n",
        "    for i in range(size):\n",
        "        sample = subsample(dataset, ratio)\n",
        "        sample_mean = mean([row[0] for row in sample])\n",
        "        sample_means.append(sample_mean)\n",
        "    print('Samples=%d, Estimated Mean: %.3f' % (size, mean(sample_means)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSqKW2rDugmR",
        "outputId": "5ea9b22a-27a8-4d2f-c574-6beaf7d30cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Mean: 4.500\n",
            "Samples=1, Estimated Mean: 4.000\n",
            "Samples=10, Estimated Mean: 4.700\n",
            "Samples=100, Estimated Mean: 4.570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed, randrange\n",
        "from csv import reader\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = len(dataset) // n_folds\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "    gini = 0.0\n",
        "    for class_value in class_values:\n",
        "        for group in groups:\n",
        "            size = len(group)\n",
        "            if size == 0:\n",
        "                continue\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "            gini += (proportion * (1.0 - proportion))\n",
        "    return gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "\n",
        "    # check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "\n",
        "    # check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "\n",
        "    # process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "\n",
        "    # process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "    sample = list()\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    while len(sample) < n_sample:\n",
        "        index = randrange(len(dataset))\n",
        "        sample.append(dataset[index])\n",
        "    return sample\n",
        "\n",
        "# Make a prediction with a list of bagged trees\n",
        "def bagging_predict(trees, row):\n",
        "    predictions = [predict(tree, row) for tree in trees]\n",
        "    return max(set(predictions), key=predictions.count)\n",
        "\n",
        "# Bootstrap Aggregation Algorithm\n",
        "def bagging(train, test, max_depth, min_size, sample_size, n_trees):\n",
        "    trees = list()\n",
        "    for i in range(n_trees):\n",
        "        sample = subsample(train, sample_size)\n",
        "        tree = build_tree(sample, max_depth, min_size)\n",
        "        trees.append(tree)\n",
        "    predictions = [bagging_predict(trees, row) for row in test]\n",
        "    return predictions\n",
        "\n",
        "# Test bagging on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = '/content/drive/MyDrive/ML_Lab-04/sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(len(dataset[0])-1):\n",
        "    str_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 6\n",
        "min_size = 2\n",
        "sample_size = 0.50\n",
        "for n_trees in [1, 5, 10, 50]:\n",
        "    scores = evaluate_algorithm(dataset, bagging, n_folds, max_depth, min_size, sample_size,n_trees)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzW9eMoAuhjG",
        "outputId": "f50b3390-5e89-4e55-cfc8-ed9842e839ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trees: 1\n",
            "Scores: [60.97560975609756, 65.85365853658537, 58.536585365853654, 63.41463414634146, 65.85365853658537]\n",
            "Mean Accuracy: 62.927%\n",
            "Trees: 5\n",
            "Scores: [65.85365853658537, 56.09756097560976, 68.29268292682927, 68.29268292682927, 53.65853658536586]\n",
            "Mean Accuracy: 62.439%\n",
            "Trees: 10\n",
            "Scores: [56.09756097560976, 63.41463414634146, 68.29268292682927, 75.60975609756098, 60.97560975609756]\n",
            "Mean Accuracy: 64.878%\n",
            "Trees: 50\n",
            "Scores: [73.17073170731707, 70.73170731707317, 68.29268292682927, 65.85365853658537, 73.17073170731707]\n",
            "Mean Accuracy: 70.244%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Chapter 18*"
      ],
      "metadata": {
        "id": "1Wf8DpzFvnxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Submodel #1: k-nearest Neighbors*"
      ],
      "metadata": {
        "id": "L66bAem3zRXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the KNN model\n",
        "def knn_model(train):\n",
        "  return train"
      ],
      "metadata": {
        "id": "sQMLkfrWvQL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Euclidean distance between two vectors\n",
        "from math import sqrt\n",
        "\n",
        "def euclidean_distance(row1, row2):\n",
        "    distance = 0.0\n",
        "    for i in range(len(row1) - 1):\n",
        "        distance += (row1[i] - row2[i]) ** 2\n",
        "    return sqrt(distance)\n",
        "\n",
        "# Locate neighbors for a new row\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    distances = list()\n",
        "    for train_row in train:\n",
        "        dist = euclidean_distance(test_row, train_row)\n",
        "        distances.append((train_row, dist))\n",
        "    distances.sort(key=lambda tup: tup[1])\n",
        "    neighbors = list()\n",
        "    for i in range(num_neighbors):\n",
        "        neighbors.append(distances[i][0])\n",
        "    return neighbors\n",
        "\n",
        "# Make a prediction with KNN\n",
        "def knn_predict(model, test_row, num_neighbors=2):\n",
        "    neighbors = get_neighbors(model, test_row, num_neighbors)\n",
        "    output_values = [row[-1] for row in neighbors]\n",
        "    prediction = max(set(output_values), key=output_values.count)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "UKJOzqlUxaDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Submodel #2: Perceptron*"
      ],
      "metadata": {
        "id": "UqLpYWiVzKSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with weights\n",
        "def perceptron_predict(model, row):\n",
        "    activation = model[0]\n",
        "    for i in range(len(row) - 1):\n",
        "        activation += model[i + 1] * row[i]\n",
        "    return 1.0 if activation >= 0.0 else 0.0\n",
        "\n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
        "    weights = [0.0 for i in range(len(train[0]))]\n",
        "    for epoch in range(n_epoch):\n",
        "        for row in train:\n",
        "            prediction = perceptron_predict(weights, row)\n",
        "            error = row[-1] - prediction\n",
        "            weights[0] = weights[0] + l_rate * error\n",
        "            for i in range(len(row) - 1):\n",
        "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "Br2itdBZxcvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Aggregator Model: Logistic Regression*"
      ],
      "metadata": {
        "id": "BrQ-2C2nxoqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def logistic_regression_predict(model, row):\n",
        "    yhat = model[0]\n",
        "    for i in range(len(row) - 1):\n",
        "        yhat += model[i + 1] * row[i]\n",
        "    return 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
        "    coef = [0.0 for i in range(len(train[0]))]\n",
        "    for epoch in range(n_epoch):\n",
        "        for row in train:\n",
        "            yhat = logistic_regression_predict(coef, row)\n",
        "            error = row[-1] - yhat\n",
        "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "            for i in range(len(row) - 1):\n",
        "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "    return coef\n"
      ],
      "metadata": {
        "id": "jpB3o55Yxs7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Combining Predictions*"
      ],
      "metadata": {
        "id": "6bQbbHx6zCoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with submodels and construct a new stacked row\n",
        "def to_stacked_row(models, predict_list, row):\n",
        "    stacked_row = list()\n",
        "    for i in range(len(models)):\n",
        "        prediction = predict_list[i](models[i], row)\n",
        "        stacked_row.append(prediction)\n",
        "    stacked_row.append(row[-1])\n",
        "    return stacked_row"
      ],
      "metadata": {
        "id": "QVquQ7Psx1Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with sub-models and construct a new stacked row\n",
        "def to_stacked_row(models, predict_list, row):\n",
        "    stacked_row = list()\n",
        "    for i in range(len(models)):\n",
        "        prediction = predict_list[i](models[i], row)\n",
        "        stacked_row.append(prediction)\n",
        "    stacked_row.append(row[-1])\n",
        "    return row[0:len(row)-1] + stacked_row"
      ],
      "metadata": {
        "id": "O9y7ug7Dx96f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Sonar Case Study*"
      ],
      "metadata": {
        "id": "3_VuL2f8yrll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "from math import exp\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = len(dataset) // n_folds\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Calculate the Euclidean distance between two vectors\n",
        "def euclidean_distance(row1, row2):\n",
        "    distance = 0.0\n",
        "    for i in range(len(row1)-1):\n",
        "        distance += (row1[i] - row2[i])**2\n",
        "    return sqrt(distance)\n",
        "\n",
        "# Locate neighbors for a new row\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    distances = list()\n",
        "    for train_row in train:\n",
        "        dist = euclidean_distance(test_row, train_row)\n",
        "        distances.append((train_row, dist))\n",
        "    distances.sort(key=lambda tup: tup[1])\n",
        "    neighbors = list()\n",
        "    for i in range(num_neighbors):\n",
        "        neighbors.append(distances[i][0])\n",
        "    return neighbors\n",
        "\n",
        "# Make a prediction with kNN\n",
        "def knn_predict(model, test_row, num_neighbors=2):\n",
        "    neighbors = get_neighbors(model, test_row, num_neighbors)\n",
        "    output_values = [row[-1] for row in neighbors]\n",
        "    prediction = max(set(output_values), key=output_values.count)\n",
        "    return prediction\n",
        "\n",
        "# Prepare the kNN model\n",
        "def knn_model(train):\n",
        "    return train\n",
        "\n",
        "# Make a prediction with weights\n",
        "def perceptron_predict(model, row):\n",
        "    activation = model[0]\n",
        "    for i in range(len(row)-1):\n",
        "        activation += model[i + 1] * row[i]\n",
        "    return 1.0 if activation >= 0.0 else 0.0\n",
        "\n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def perceptron_model(train, l_rate=0.01, n_epoch=5000):\n",
        "    weights = [0.0 for i in range(len(train[0]))]\n",
        "    for epoch in range(n_epoch):\n",
        "        for row in train:\n",
        "            prediction = perceptron_predict(weights, row)\n",
        "            error = row[-1] - prediction\n",
        "            weights[0] = weights[0] + l_rate * error\n",
        "            for i in range(len(row)-1):\n",
        "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "    return weights\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def logistic_regression_predict(model, row):\n",
        "    yhat = model[0]\n",
        "    for i in range(len(row)-1):\n",
        "        yhat += model[i + 1] * row[i]\n",
        "    return 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def logistic_regression_model(train, l_rate=0.01, n_epoch=5000):\n",
        "    coef = [0.0 for i in range(len(train[0]))]\n",
        "    for epoch in range(n_epoch):\n",
        "        for row in train:\n",
        "            yhat = logistic_regression_predict(coef, row)\n",
        "            error = row[-1] - yhat\n",
        "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "            for i in range(len(row)-1):\n",
        "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "    return coef\n",
        "\n",
        "# Make predictions with sub-models and construct a new stacked row\n",
        "def to_stacked_row(models, predict_list, row):\n",
        "    stacked_row = list()\n",
        "    for i in range(len(models)):\n",
        "        prediction = predict_list[i](models[i], row)\n",
        "        stacked_row.append(prediction)\n",
        "    stacked_row.append(row[-1])\n",
        "    return row[0:len(row)-1] + stacked_row\n",
        "\n",
        "# Stacked Generalization Algorithm\n",
        "def stacking(train, test):\n",
        "    model_list = [knn_model, perceptron_model]\n",
        "    predict_list = [knn_predict, perceptron_predict]\n",
        "    models = list()\n",
        "    for i in range(len(model_list)):\n",
        "        model = model_list[i](train)\n",
        "        models.append(model)\n",
        "    stacked_dataset = list()\n",
        "    for row in train:\n",
        "        stacked_row = to_stacked_row(models, predict_list, row)\n",
        "        stacked_dataset.append(stacked_row)\n",
        "    stacked_model = logistic_regression_model(stacked_dataset)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        stacked_row = to_stacked_row(models, predict_list, row)\n",
        "        stacked_dataset.append(stacked_row)\n",
        "        prediction = logistic_regression_predict(stacked_model, stacked_row)\n",
        "        prediction = round(prediction)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "# Test stacking on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = '/content/drive/MyDrive/ML_Lab-04/sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string\n",
        "for i in range(len(dataset[0])-1):\n",
        "  str_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "n_folds = 3\n",
        "scores = evaluate_algorithm(dataset, stacking, n_folds)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUDf7I4GyuKA",
        "outputId": "562ad59b-2c1e-478f-b2dc-cb3042ac470a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [78.26086956521739, 76.81159420289855, 69.56521739130434]\n",
            "Mean Accuracy: 74.879%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cim_2AIZy0bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}